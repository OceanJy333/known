import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
  baseURL: process.env.OPENAI_BASE_URL
})

// æµå¼å“åº”ç¼–ç å™¨
class StreamEncoder {
  static encode(type: string, data: any): string {
    return `data: ${JSON.stringify({ type, data, timestamp: new Date().toISOString() })}\n\n`
  }

  static error(message: string): string {
    return `data: ${JSON.stringify({ type: 'error', message, timestamp: new Date().toISOString() })}\n\n`
  }

  static complete(): string {
    return `data: ${JSON.stringify({ type: 'complete', timestamp: new Date().toISOString() })}\n\n`
  }
}

export async function POST(request: NextRequest) {
  try {
    const body = await request.json()
    const { 
      question, 
      contextCards = [], 
      nodeId,
      conversationHistory = [],
      nodeType = 'ai-chat'
    } = body

    console.log('ğŸš€ [è¾“å‡ºèŠ‚ç‚¹API] æ”¶åˆ°è¯·æ±‚:', {
      question: question?.slice(0, 100) + '...',
      contextCardsSize: contextCards.length,
      nodeId,
      nodeType,
      hasHistory: conversationHistory.length > 0,
      contextCardsSample: contextCards.slice(0, 3).map((note: any) => ({
        id: note.id,
        title: note.title,
        summary: note.summary?.slice(0, 50) + '...'
      }))
    })

    // åˆ›å»ºæµå¼å“åº”
    const encoder = new TextEncoder()
    const stream = new ReadableStream({
      async start(controller) {
        try {
          // å‘é€å¼€å§‹äº‹ä»¶
          controller.enqueue(encoder.encode(StreamEncoder.encode('start', { nodeId, question })))

          // ç›´æ¥ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å¡ç‰‡è¿›è¡Œå¯¹è¯
          console.log('ğŸ¤– [AIå¯¹è¯] å¼€å§‹ç”Ÿæˆå›ç­”')
          controller.enqueue(encoder.encode(StreamEncoder.encode('status', { 
            status: 'generating', 
            message: 'æ­£åœ¨ç”Ÿæˆå›ç­”...' 
          })))

          await generateAnswerWithContext(
            question, 
            contextCards, 
            conversationHistory, 
            nodeType,
            controller, 
            encoder
          )

          // å‘é€å®Œæˆäº‹ä»¶
          controller.enqueue(encoder.encode(StreamEncoder.encode('complete', { 
            nodeId, 
            contextUsed: contextCards.length 
          })))

        } catch (error) {
          console.error('âŒ [è¾“å‡ºèŠ‚ç‚¹API] å¤„ç†å¤±è´¥:', error)
          controller.enqueue(encoder.encode(StreamEncoder.error(
            error instanceof Error ? error.message : 'å¤„ç†è¯·æ±‚æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯'
          )))
        } finally {
          controller.close()
        }
      }
    })

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    })

  } catch (error) {
    console.error('âŒ [è¾“å‡ºèŠ‚ç‚¹API] åˆå§‹åŒ–å¤±è´¥:', error)
    return NextResponse.json(
      { error: 'è¯·æ±‚å¤„ç†å¤±è´¥' },
      { status: 500 }
    )
  }
}


// åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
async function generateAnswerWithContext(
  question: string,
  contextCards: any[],
  conversationHistory: any[],
  nodeType: string,
  controller: ReadableStreamDefaultController,
  encoder: TextEncoder
) {
  
  // æ„å»ºä¸Šä¸‹æ–‡
  const contextContent = contextCards.map(card => `
æ ‡é¢˜ï¼š${card.title}
æ‘˜è¦ï¼š${card.summary || 'æš‚æ— æ‘˜è¦'}
å†…å®¹ï¼š${card.content ? card.content.slice(0, 500) + '...' : 'æš‚æ— è¯¦ç»†å†…å®¹'}
æ ‡ç­¾ï¼š${card.tags?.join(', ') || 'æ— '}
`).join('\n---\n')

  // æ„å»ºå¯¹è¯å†å²
  const historyContent = conversationHistory
    .slice(-4) // åªä¿ç•™æœ€è¿‘4è½®å¯¹è¯
    .map((msg: any) => `${msg.role === 'user' ? 'ç”¨æˆ·' : 'AI'}ï¼š${msg.content}`)
    .join('\n')

  // æ ¹æ®èŠ‚ç‚¹ç±»å‹é€‰æ‹©ç³»ç»Ÿæç¤º
  const systemPrompts = {
    'ai-chat': `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†åŠ©æ‰‹ã€‚åŸºäºç”¨æˆ·æä¾›çš„çŸ¥è¯†å¡ç‰‡å†…å®¹å’Œå¯¹è¯å†å²ï¼Œä¸ºç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚

å›ç­”è¦æ±‚ï¼š
1. å……åˆ†åˆ©ç”¨æä¾›çš„çŸ¥è¯†å¡ç‰‡å†…å®¹
2. è€ƒè™‘å¯¹è¯å†å²çš„ä¸Šä¸‹æ–‡
3. ä¿æŒå®¢è§‚ã€å‡†ç¡®çš„è¯­è°ƒ
4. æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®
5. å¦‚æœçŸ¥è¯†å¡ç‰‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜å¹¶æä¾›é€šç”¨å»ºè®®
6. é€‚å½“å¼•ç”¨çŸ¥è¯†å¡ç‰‡ä¸­çš„å…³é”®ä¿¡æ¯`,
    
    'html-page': 'åŸºäºçŸ¥è¯†å¡ç‰‡å†…å®¹ç”Ÿæˆå®Œæ•´çš„HTMLç½‘é¡µ...',
    'xiaohongshu': 'åŸºäºçŸ¥è¯†å¡ç‰‡å†…å®¹ç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„å›¾æ–‡å†…å®¹...'
  }

  const systemPrompt = systemPrompts[nodeType as keyof typeof systemPrompts] || systemPrompts['ai-chat']

  const answerPrompt = `
${contextCards.length > 0 ? `å¯ç”¨çŸ¥è¯†å¡ç‰‡ï¼š\n${contextContent}\n` : ''}

${conversationHistory.length > 0 ? `å¯¹è¯å†å²ï¼š\n${historyContent}\n` : ''}

ç”¨æˆ·é—®é¢˜ï¼š${question}

è¯·åŸºäºä¸Šè¿°ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚`

  try {
    const answerResponse = await openai.chat.completions.create({
      model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: answerPrompt }
      ],
      stream: true,
      temperature: 0.7,
      max_tokens: 2000
    })

    // æµå¼å‘é€å›ç­”å†…å®¹
    let fullAnswer = ''
    for await (const chunk of answerResponse) {
      const content = chunk.choices[0]?.delta?.content
      if (content) {
        fullAnswer += content
        console.log('ğŸ“¤ [API] å‘é€å†…å®¹ç‰‡æ®µ:', { 
          contentLength: content.length,
          totalLength: fullAnswer.length,
          preview: content.slice(0, 50) + '...'
        })
        controller.enqueue(encoder.encode(StreamEncoder.encode('answer_content', { content })))
      }
    }

    console.log('âœ… [ç”Ÿæˆå›ç­”] å®Œæˆ:', {
      answerLength: fullAnswer.length,
      contextCardsUsed: contextCards.length
    })

  } catch (error) {
    console.error('âŒ [ç”Ÿæˆå›ç­”] å¤±è´¥:', error)
    throw error
  }
}